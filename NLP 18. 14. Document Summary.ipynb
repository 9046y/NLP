{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "readable_title=''\n",
    "def download_document(url):\n",
    "    \"\"\"Downloads document using BeautifulSoup, extracts the subject and all\n",
    "    text stored in paragraph tags\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    readable_title = soup.find('title').get_text()\n",
    "    document = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return document\n",
    "\n",
    "url = \"http://venturebeat.com/2014/07/04/facebooks-little-social-experiment-got-you-bummed-out-get-over-it/\"\n",
    "document = download_document(url)\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis\n",
    "Here's a little secret: much of NLP (and data science, for that matter) boils down to counting things. If you've got a bunch of data that needs *analyzin'* but you don't know where to start, counting things is usually a good place to begin. Sure, you'll need to figure out exactly what you want to count, how to count it, and what to do with the counts, but if you're lost and don't know what to do, **just start counting**.\n",
    "\n",
    "Perhaps we'd like to begin (as is often the case in NLP) by examining the words that appear in our document. To do that, we'll first need to tokenize the text string into discrete words. Since we're working with English, this isn't so bad, but if we were working with a non-whitespace-delimited language like Chinese, Japanese, or Korean, it would be much more difficult.\n",
    "\n",
    "Notice that the output contains some punctuation & numbers, hasn't been loweredcased, and counts *BuzzFeed* and *BuzzFeed's* separately. We'll tackle some of those issues next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( [1]\n",
      ") [1]\n",
      ", [28]\n",
      ". [42]\n",
      "2012 [1]\n",
      "700,000 [3]\n",
      ": [5]\n",
      "; [1]\n",
      "? [9]\n",
      "A/B [2]\n",
      "According [2]\n",
      "Actually [1]\n",
      "After [2]\n",
      "All [1]\n",
      "And [3]\n",
      "Before [1]\n",
      "Believe [1]\n",
      "Blame [1]\n",
      "But [2]\n",
      "BuzzFeed’s [1]\n",
      "Buzzfeed [1]\n",
      "Could [1]\n",
      "Count [1]\n",
      "David [1]\n",
      "Did [2]\n",
      "Don’t [1]\n",
      "Editorial [1]\n",
      "Epic [1]\n",
      "Facebook [13]\n",
      "Facebook’s [4]\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for sent in nltk.sent_tokenize(document) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "for token in sorted(set(tokens))[:30]:\n",
    "    print (token + ' [' + str(tokens.count(token)) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Stemming\n",
    "[Stemming](http://en.wikipedia.org/wiki/Stemming) is the process of reducing a word to its base/stem/root form. Most stemmers are pretty basic and just chop off standard affixes indicating things like tense (e.g., \"-ed\") and possessive forms (e.g., \"-'s\"). Here, we'll use the Snowball stemmer for English, which comes with NLTK.\n",
    "\n",
    "Once our tokens are stemmed, we can rest easy knowing that *BuzzFeed* and *BuzzFeed's* are now being counted together as... *buzzfe*? Don't worry: although this may look weird, it's pretty standard behavior for stemmers and won't affect our analysis (much). We also (probably) won't show the stemmed words to users -- we'll normally just use them for internal analysis or indexing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior [1]\n",
      "believ [1]\n",
      "besid [1]\n",
      "better [1]\n",
      "blame [2]\n",
      "breach [1]\n",
      "built [1]\n",
      "bum [1]\n",
      "but [4]\n",
      "button [1]\n",
      "buzzfe [2]\n",
      "by [5]\n",
      "came [2]\n",
      "can [3]\n",
      "cat [1]\n",
      "charg [1]\n",
      "children [1]\n",
      "citi [1]\n",
      "click [1]\n",
      "come-on [1]\n",
      "compani [2]\n",
      "condit [1]\n",
      "confidenti [1]\n",
      "connect [1]\n",
      "contagion.” [1]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "for token in sorted(set(stemmed_tokens))[50:75]:\n",
    "    print (token + ' [' + str(stemmed_tokens.count(token)) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Although the stemmer very helpfully chopped off pesky affixes (and made everything lowercase to boot), there are some word forms that give stemmers indigestion, especially *irregular* words. While the process of stemming typically involves rule-based methods of stripping affixes (making them small & fast), **lemmatization** involves dictionary-based methods to derive the canonical forms (i.e., *lemmas*) of words. For example, *run*, *runs*, *ran*, and *running* all correspond to the lemma *run*. However, lemmatizers are generally big, slow, and brittle due to the nature of the dictionary-based methods, so you'll only want to use them when necessary.\n",
    "\n",
    "The example below compares the output of the Snowball stemmer with the WordNet lemmatizer (also distributed with NLTK). Notice that the lemmatizer correctly converts *women* into *woman*, while the stemmer turns *lying* into *lie*. Additionally, both replace *eyes* with *eye*, but neither of them properly transforms *told* into *tell*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sever', 'women', 'told', 'me', 'i', 'have', 'lie', 'eye', '.']\n",
      "['Several', 'woman', 'told', 'me', 'I', 'have', 'lying', 'eye', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "temp_sent = \"Several women told me I have lying eyes.\"\n",
    "\n",
    "print ([stemmer.stem(t) for t in nltk.word_tokenize(temp_sent)])\n",
    "print ([lemmatizer.lemmatize(t) for t in nltk.word_tokenize(temp_sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Frequency Distributions\n",
    "Thus far, we've been working with lists of tokens that we're manually sorting, uniquifying, and counting -- all of which can get to be a bit cumbersome. Fortunately, NLTK provides a data structure called <code>FreqDist</code> that makes it more convenient to work with these kinds of frequency distributions. The code snippet below builds a <code>FreqDist</code> from our list of stemmed tokens, and then displays the top 25 tokens appearing most frequently in the text of our article. Wasn't that easy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 44)\n",
      "('.', 42)\n",
      "('of', 29)\n",
      "(',', 28)\n",
      "('to', 22)\n",
      "('it', 21)\n",
      "('that', 20)\n",
      "('facebook', 17)\n",
      "('and', 16)\n",
      "('you', 14)\n",
      "('a', 13)\n",
      "('is', 11)\n",
      "('in', 11)\n",
      "('be', 11)\n",
      "('content', 10)\n",
      "('they', 10)\n",
      "('user', 9)\n",
      "('?', 9)\n",
      "('if', 9)\n",
      "('their', 8)\n",
      "('happen', 8)\n",
      "('was', 7)\n",
      "('would', 6)\n",
      "('all', 6)\n",
      "('this', 6)\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(stemmed_tokens)\n",
    "\n",
    "#for item in list(fdist.items())[:25]:\n",
    "for item in fdist.most_common(25):\n",
    "     print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering out Stop Words\n",
    "Notice in the output above that most of the top 25 tokens are worthless. With the exception of things like *facebook*, *content*, *user*, and perhaps *emot* (emotion?), the rest are basically devoid of meaningful information. They don't really tells us anything about the article since these tokens will appear is just about any English document. What we need to do is filter out these [*stop words*](http://en.wikipedia.org/wiki/Stop_words) in order to focus on just the important material.\n",
    "\n",
    "While there is no single, definitive list of stop words, NLTK provides a decent start. Let's load it up and take a look at what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(nltk.corpus.stopwords.words('english'))[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this list to filter-out stop words from our list of stemmed tokens before we create the frequency distribution. You'll notice in the output below that we still have some things like punctuation that we'd probably like to remove, but we're much closer to having a list of the most \"important\" words in our article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', 42)\n",
      "(',', 28)\n",
      "('facebook', 17)\n",
      "('content', 10)\n",
      "('?', 9)\n",
      "('user', 9)\n",
      "('happen', 8)\n",
      "('would', 6)\n",
      "('emot', 6)\n",
      "('peopl', 6)\n",
      "('feed', 5)\n",
      "(':', 5)\n",
      "('research', 5)\n",
      "('privaci', 5)\n",
      "('friend', 5)\n",
      "('test', 5)\n",
      "('negat', 5)\n",
      "('posit', 5)\n",
      "('one', 5)\n",
      "('use', 5)\n",
      "('could', 4)\n",
      "('might', 4)\n",
      "('everi', 4)\n",
      "('time', 4)\n",
      "('read', 4)\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "fdist2 = nltk.FreqDist(stemmed_tokens_no_stop)\n",
    "\n",
    "#for item in fdist2.items()[:25]:\n",
    "for item in fdist2.most_common(25):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "Another task we might want to do to help identify what's \"important\" in a text document is [named entity recogniton (NER)](http://en.wikipedia.org/wiki/Named-entity_recognition). Also called *entity extraction*, this process involves automatically extracting the names of persons, places, organizations, and potentially other entity types out of unstructured text. Building an NER classifier requires *lots* of annotated training data and some [fancy machine learning algorithms](http://en.wikipedia.org/wiki/Conditional_random_field), but fortunately, NLTK comes with a pre-built/pre-trained NER classifier ready to extract entities right out of the box. This classifier has been trained to recognize PERSON, ORGANIZATION, and GPE (geo-political entity) entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PERSON] Charlie\n",
      "[ORGANIZATION] Altamira\n",
      "[GPE] Tysons Corner\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "\tentities = []\n",
    "\tfor sentence in nltk.sent_tokenize(text):\n",
    "\t    chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "\t    entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "\treturn entities\n",
    "\n",
    "for entity in extract_entities('My name is Charlie and I work for Altamira in Tysons Corner.'):\n",
    "    print ('[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Summarization\n",
    "\n",
    "The Reuters Corpus contains nearly 11,000 news articles about a variety of topics and subjects. If you've run the <code>nltk.download()</code> command as previously recommended, you can then easily import and explore the Reuters Corpus like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** BEGIN ARTICLE: ** \"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Japan has raised fears among many of Asia's exporting\n",
      "  nations that the row could inflict far-reaching economic\n",
      "  damage, businessmen and officials said.\n",
      "      They told Reuter correspondents in Asian capitals a U.S.\n",
      "  Move against Japan might boost protectionist sentiment in the\n",
      "  U.S. And lead to curbs on American imports of their products.\n",
      "      But some exporters said that while the conflict wo [...]\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "print ('** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[0])[:500] + ' [...]\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- assign a score to each word in a document corresponding to its level of \"importance\"\n",
    "- rank each sentence in the document by summing the individual word scores and dividing by the number of tokens in the sentence\n",
    "- extract the top N highest scoring sentences and return them as our \"summary\"\n",
    "\n",
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Consider a document that contains the word *baseball* 8 times. You might think, \"wow, *baseball* isn't a stop word, and it appeared rather frequently here, so it's probably important.\" And you might be right. But what if that document is actually an article posted on a baseball blog? Won't the word *baseball* appear frequently in nearly every post on that blog? In this particular case, if you were generating a summary of this document, would the word *baseball* be a good indicator of importance, or would you maybe look for other words that help distinguish or differentiate this blog post from the rest?\n",
    "\n",
    "Context is essential. What really matters here isn't the raw frequency of the number of times each word appeared in a document, but rather the **relative frequency** comparing the number of times a word appeared in this document against the number of times it appeared across the rest of the collection of documents. \"Important\" words will be the ones that are generally rare across the collection, but which appear with an unusually high frequency in a given document.\n",
    "\n",
    "We'll calculate this relative frequency using a statistical metric called [term frequency - inverse document frequency (TF-IDF)](http://en.wikipedia.org/wiki/Tf%E2%80%93idf). We could use the TF-IDF implementation provided by the [scikit-learn](http://scikit-learn.org/) machine learning library for Python.\n",
    "\n",
    "#### Building a Term-Document Matrix\n",
    "\n",
    "We'll use scikit-learn's <code>TfidfVectorizer</code> class to construct a [term-document matrix](http://en.wikipedia.org/wiki/Document-term_matrix) containing the TF-IDF score for each word in each document in the Reuters Corpus. In essence, the rows of this sparse matrix correspond to documents in the corpus, the columns represent each word in the vocabulary of the corpus, and each cell contains the TF-IDF value for a given word in a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building term-document matrix... [process started: 2018-04-25 01:01:52.435330]\n",
      "done! [process finished: 2018-04-25 01:03:01.124108]\n"
     ]
    }
   ],
   "source": [
    "import datetime, re, sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "token_dict = {}\n",
    "for article in reuters.fileids():\n",
    "    token_dict[article] = reuters.raw(article)\n",
    "        \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
    "print ('building term-document matrix... [process started: ' + str(datetime.datetime.now()) + ']')\n",
    "sys.stdout.flush()\n",
    "\n",
    "tdm = tfidf.fit_transform(token_dict.values()) # this can take some time (about 60 seconds on my machine)\n",
    "print ('done! [process finished: ' + str(datetime.datetime.now()) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Scores\n",
    "\n",
    "Now that we've built the term-document matrix, we can explore its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM contains 25833 terms and 10788 documents\n",
      "first term: 'd\n",
      "last term: zzzz\n",
      "random term: mcfadden\n",
      "random term: belcher\n",
      "random term: baord\n",
      "random term: sarji\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print ('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')\n",
    "\n",
    "print ('first term: ' + feature_names[0])\n",
    "print ('last term: ' + feature_names[len(feature_names) - 1])\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print ('random term: ' + feature_names[randint(1,len(feature_names) - 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the Summary\n",
    "\n",
    "That's all we'll need to produce a summary for any document in the corpus. In the example code below, we start by randomly selecting an article from the Reuters Corpus. We iterate through the article, calculating a score for each sentence by summing the TF-IDF values for each word appearing in the sentence. We normalize the sentence scores by dividing by the number of tokens in the sentence (to avoid bias in favor of longer sentences). Then we sort the sentences by their scores, and return the highest-scoring sentences as our summary. The number of sentences returned corresponds to roughly 20% of the overall length of the article.\n",
    "\n",
    "Since some of the articles in the Reuters Corpus are rather small (i.e., a single sentence in length) or contain just raw financial data, some of the summaries won't make sense. If you run this code a few times, however, you'll eventually see a randomly-selected article that provides a decent demonstration of this simplistic method of identifying the \"most important\" sentence from a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SUMMARY ***\n",
      "Hard-hit by the collapse in oil and Texas real estate\n",
      "  prices, First City's net loan chargeoffs totaled 366 mln dlrs\n",
      "  last year, up from 261 mln dlrs in 1985.\n",
      "The banks agreed to similar amendments to the covenants\n",
      "  last year and First City has reduced its borrowings from 120\n",
      "  mln dlrs at 1986 yearend to 68.5 mln dlrs in recent weeks.\n",
      "The bank more than\n",
      "  doubled its loan loss provision to 497 mln dlrs at the end of\n",
      "  1986.\n",
      "In real estate, First City said its nonperforming assets\n",
      "  nearly doubled last year to 347 mln dlrs at year-end.\n",
      "AUDITORS GIVE FIRST CITY &lt;FBT> QUALIFIED OPINION\n",
      "  First City Bancorp of Texas, which lost\n",
      "  a record 402 mln dlrs in 1986, said in its annual report it\n",
      "  expected operating losses to continue \"for the foreseeable\n",
      "  future\" as it continues to search for additional capital or a\n",
      "  merger partner.\n",
      "\n",
      "*** ORIGINAL ***\n",
      "AUDITORS GIVE FIRST CITY &lt;FBT> QUALIFIED OPINION\n",
      "  First City Bancorp of Texas, which lost\n",
      "  a record 402 mln dlrs in 1986, said in its annual report it\n",
      "  expected operating losses to continue \"for the foreseeable\n",
      "  future\" as it continues to search for additional capital or a\n",
      "  merger partner.\n",
      "      The Houston-based bank's 1986 financial statements received\n",
      "  a qualified opinion from its auditors, Arthur Andersen and Co.\n",
      "  The auditors said their opinion was subject to First City\n",
      "  eventually obtaining additional capital.\n",
      "      \"The company believes that in order to address its\n",
      "  long-term needs and return to a satisfactory level of\n",
      "  operations, it will ultimately need several hundred million\n",
      "  dollars of additional capital, or a combination with a more\n",
      "  strongly capitalized entity,\" First City said in a note to its\n",
      "  financial statements included in the annual report.\n",
      "      \"Management believes that sufficient resources should be\n",
      "  available to cover interim capital concerns while additional\n",
      "  capital is being sought,\" the bank said.\n",
      "      To raise cash in the near-term, First City said it may sell\n",
      "  or mortgage non-strategic assets, recover excess contributions\n",
      "  to its pension plan and obtain special dividends from some of\n",
      "  its member banks.\n",
      "      \"The losses for 1987 are expected to be substantially less\n",
      "  than in 1986,\" First City chairman J.A. Elkins said in a letter\n",
      "  included in the annual report. \"However, the ultimate return to\n",
      "  satisfactory operating conditions is dependent on the\n",
      "  successful resolution of the related problems of credit\n",
      "  quality, funding and the eventual need for substantial\n",
      "  additional capital.\"\n",
      "      First City said it anticipated that certain covenants of a\n",
      "  credit agreement with unaffiliated banks requiring most of\n",
      "  First City's excess cash to be applied to debt repayments would\n",
      "  be modified by the end of the first quarter in order to avoid\n",
      "  default.\n",
      "      The banks agreed to similar amendments to the covenants\n",
      "  last year and First City has reduced its borrowings from 120\n",
      "  mln dlrs at 1986 yearend to 68.5 mln dlrs in recent weeks.\n",
      "      Although the parent company's capital adequacy ratios\n",
      "  exceeded regulatory minimum requirements at the end of 1986,\n",
      "  First City said its two largest subsidiaries did not. First\n",
      "  City National Bank of Houston had a primary capital ratio of\n",
      "  5.34 pct and First City Bank of Dallas had a 4.75 pct ratio.\n",
      "      Hard-hit by the collapse in oil and Texas real estate\n",
      "  prices, First City's net loan chargeoffs totaled 366 mln dlrs\n",
      "  last year, up from 261 mln dlrs in 1985. The bank more than\n",
      "  doubled its loan loss provision to 497 mln dlrs at the end of\n",
      "  1986.\n",
      "      First City said chargeoffs and paydowns reduced its total\n",
      "  energy loan portfolio by 32 pct during 1986, to 1.4 billion\n",
      "  dlrs at year-end, adding that future energy chargeoffs \"should\n",
      "  be more modest.\" The amount represented 15 pct of First City's\n",
      "  total loans.\n",
      "      In real estate, First City said its nonperforming assets\n",
      "  nearly doubled last year to 347 mln dlrs at year-end.\n",
      "  Chargeoffs of real estate loans rose to 32 mln dlrs, or nine\n",
      "  pct of total loan chargeoffs, and the bank said the amount\n",
      "  could go higher.\n",
      "      \"The company still faces uncertainties in the real estate\n",
      "  market and anticipates further deterioration in the pportfolio\n",
      "  so long as the regional recession persists,\" First City said.\n",
      "  \"Because the carrying value of many of these loans is\n",
      "  collateral dependent, a further decline in the overall value of\n",
      "  the collateral base could cause an increase in the level of\n",
      "  real estate-related chargeoffs.\"\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "article_id = randint(0, tdm.shape[0] - 1)\n",
    "article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "\n",
    "print ('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    print (summary_sentence[1])\n",
    "\n",
    "print ('\\n*** ORIGINAL ***')\n",
    "print (article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
