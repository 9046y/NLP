{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automatic Summarization\n",
    "\n",
    "The Reuters Corpus contains nearly 11,000 news articles about a variety of topics and subjects.\n",
    "If you've run the <code>nltk.download()</code> command as previously recommended, you can then \n",
    "easily import and explore the Reuters Corpus like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "print ('** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[0])[:500] + ' [...]\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "token_dict = {}\n",
    "for article in reuters.fileids():\n",
    "    token_dict[article] = reuters.raw(article)\n",
    "        \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(token_dict.values())\n",
    "tdm = tfidf.transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print ('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')\n",
    "\n",
    "print ('first term: ' + feature_names[0])\n",
    "print ('last term: ' + feature_names[len(feature_names) - 1])\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print ('random term: ' + feature_names[randint(1,len(feature_names) - 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "article_id = randint(0, tdm.shape[0] - 1)\n",
    "article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "print ('\\n*** ORIGINAL ***')\n",
    "print (article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    score = 0\n",
    "    sent_tokens = tokens = [word for sent in nltk.sent_tokenize(sentence) for word in nltk.word_tokenize(sentence)]\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "\n",
    "print ('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    print (summary_sentence[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
